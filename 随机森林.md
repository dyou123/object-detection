医疗图像分割——Random Forest，GBDT，U-Net，MRF
=================================

Decision Tree，boosting，bagging，FCN，MRF CRF
----------------------------------

https://zhuanlan.zhihu.com/p/22308032

很关键的一篇关于FCN的文章



前端使用FCN进行特征粗提取，后端使用CRF/MRF优化前端的输出，最后得到分割图。

![img](/Users/duanyou/Desktop/object-detection/media/3adeadf2a20b0cc9cd68553a95f00552_hd.png)

<http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html>

**我的硕士论文是关于RF+CNN，U-Net+MRF实现医疗图像分割技术。**

**此篇文章会复习RF，GBDT，U-Net，MRF等知识。**



学习一个中文论文，https://wenku.baidu.com/view/9d2f4c8c6529647d2728522e.html

主要是最大似然估计，期望最大化算法（EM）。

自助法重采样，bootstrap，在N的数据集中，有放回的抽取N个样本；独立抽取k次，生成k个相互独立的自助样本集。

随机森林通过自助法生成k个自助样本集，每个自助样本是每棵树的全部训练数据。对于每棵分类树，在树的每个节点处，从M个特征随机挑选m个特征，一般m是M开方。按照**节点不纯度最小原则**从m个特征中选出一个特征进行生长。使得每个节点的不纯度达到最小。

每次抽样生成自助样本集，全体样本不在自助样本中的剩余样本成为out-of-bag，大约占1/3，用作预测分类正确率。汇总每棵树的OOB估计，得到分类器的正确率。

**关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。 **

对于分类问题（将某个样本划分到某一类），也就是离散变量问题，CART使用Gini值作为评判标准。定义为**Gini=1-∑(P(i)*P(i))**,P(i)为当前节点上数据集中第i类样本的比例。例如：分为2类，当前节点上有100个样本，属于第一类的样本有70个，属于第二类的样本有30个，则Gini=1-0.7×07-0.3×03=0.42，可以看出，类别分布越平均，Gini值越大，类分布越不均匀，Gini值越小。

在寻找最佳的分类特征和阈值时，评判标准为：argmax（Gini-GiniLeft-GiniRight），即寻找最佳的特征f和阈值th，使得当前节点的Gini值减去左子节点的Gini和右子节点的Gini值最大。

https://www.cnblogs.com/nxld/p/6374945.html

###**应用**

在随机森林算法的函数**randomForest()中有两个非常重要的参数**，而这两个参数又将影响模型的准确性，它们分别是**mtry和ntree**。**一般对mtry的选择是逐一尝试，直到找到比较理想的值，ntree的选择可通过图形大致判断模型内误差稳定时的值。**

https://blog.csdn.net/step_forward_ML/article/details/80519698



决策树 
----------------------------------
** 学习什么是决策树，以及运算（机器学习每周课程） **

优点：训练时间复杂度低，预测的过程快速，决策树可视化好，
缺点：overfitting
常见的模型组合形式：Boosting，Bagging组合形成RF和GBDT。

从西瓜书学习决策树：

最主要的概念：信息熵，是度量样本集合纯度最常用的一种指标。

information entropy 信息熵

![avatar](media/shang.png)

information gain 信息增益

![avatar](media/entropy2.png)

一般来说，信息增益越大，意味着使用属性a来进行划分所获的的纯度提升越大。

另外，基尼系数Gini，是一种与信息熵类似的做特征选择的方式。检测数据的纯度。

![avatar](media/gini.png)

### 剪枝
剪枝处理，pruning，是对付过拟合的主要手段。

包括预剪枝，后剪枝。

预剪枝，是指在决策树生成过程中，对每个结点在划分前进行评估，若当前划分不能给决策树带来泛化性提高，则停止划分，将当前结点作为叶结点。

后剪枝，先从训练集生成一颗完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树性能提升，则将该子树替换为叶结点。

### 如何判断决策树泛化性能提升了？

![avatar](media/jian.png)

看划分前后验证集的精度是否有提升。1划分后高，所以要划分，23划分后减小了，所以不划分。

而且，后剪枝比预剪枝决策树保留了更多的分支，欠拟合风险小，泛化好。但时间久。

随机森林 GBDT
----------------------------------

主要：information gain，决策树

决策树实际上是将空间用超平面进行划分的一种方法。

![avatar](media/dt1.png)

![avatar](media/dt2.png)

这样使得每一个叶子节点都是在空间中的一个不相交的区域。

Random forest
----------------------------------

用随机的方式建立一个森林。

建立决策树的时候，需要注意：采样，完全分裂。

RF对输入数据要进行行列采样，对于行采样，是有放回。

假设输入样本是N，采样样本也是N。这样训练时候，每棵树的输入都不是全部样本，相对不容易出现过拟合。

再进行列采样，M个feature里选择m个。之后就是对采样之后的数据使用完全分裂方式建立决策树，这样决策树的某个叶子要么无法完全分裂，要么所有样本都是指向同一个分类。

一般很多决策树算法都有一个重要的步骤，是剪枝。 


U-Net
-----------------------------------
U-Net，先学习FCN的概念，再学习U-Net的网络设计以及实现效果。

MRF
----------------------------------
在图像分割领域，为何很强？

通过计算每个体素与已知类别的距离，考虑相邻体素的影响迭代更新产生已知类别的距离，引入最大后验概率准则，将最大似然估计问题转化为类概率与类条件概率乘积的最大问题，进行最后分类。MRF可以能有效避免噪声的影响，得到平滑分割结果。





# 论文

通过使用随机森林分类器的结果来初始化CRF的一元势，然后从训练数据中学习成对势。