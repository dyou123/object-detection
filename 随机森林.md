Random Forest，GBDT，U-Net，MRF
=================================

Decision Tree，boosting，bagging，FCN，MRF CRF
----------------------------------

<http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html>

**我的硕士论文是关于RF+CNN，U-Net+MRF实现医疗图像分割技术。**

**此篇文章会复习RF，GBDT，U-Net，MRF等知识。**

决策树 
----------------------------------
** 学习什么是决策树，以及运算（机器学习每周课程） **

优点：训练时间复杂度低，预测的过程快速，决策树可视化好，
缺点：overfitting
常见的模型组合形式：Boosting，Bagging组合形成RF和GBDT。

从西瓜书学习决策树：

最主要的概念：信息熵，是度量样本集合纯度最常用的一种指标。

information entropy 信息熵

![avatar](media/shang.png)

information gain 信息增益

![avatar](media/entropy2.png)

一般来说，信息增益越大，意味着使用属性a来进行划分所获的的纯度提升越大。

另外，基尼系数Gini，是一种与信息熵类似的做特征选择的方式。检测数据的纯度。

![avatar](media/gini.png)

### 剪枝
剪枝处理，pruning，是对付过拟合的主要手段。

包括预剪枝，后剪枝。

预剪枝，是指在决策树生成过程中，对每个结点在划分前进行评估，若当前划分不能给决策树带来泛化性提高，则停止划分，将当前结点作为叶结点。

后剪枝，先从训练集生成一颗完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树性能提升，则将该子树替换为叶结点。

### 如何判断决策树泛化性能提升了？

![avatar](media/jian.png)

看划分前后验证集的精度是否有提升。1划分后高，所以要划分，23划分后减小了，所以不划分。

而且，后剪枝比预剪枝决策树保留了更多的分支，欠拟合风险小，泛化好。但时间久。

随机森林 GBDT
----------------------------------

主要：information gain，决策树

决策树实际上是将空间用超平面进行划分的一种方法。

![avatar](media/dt1.png)

![avatar](media/dt2.png)

这样使得每一个叶子节点都是在空间中的一个不相交的区域。

Random forest：
----------------------------------

用随机的方式建立一个森林。

建立决策树的时候，需要注意：采样，完全分裂。

RF对输入数据要进行行列采样，对于行采样，是有放回。

假设输入样本是N，采样样本也是N。这样训练时候，每棵树的输入都不是全部样本，相对不容易出现过拟合。

再进行列采样，M个feature里选择m个。之后就是对采样之后的数据使用完全分裂方式建立决策树，这样决策树的某个叶子要么无法完全分裂，要么所有样本都是指向同一个分类。

一般很多决策树算法都有一个重要的步骤，是剪枝。 


U-Net
-----------------------------------
U-Net，先学习FCN的概念，再学习U-Net的网络设计以及实现效果。

MRF
----------------------------------
在图像分割领域，为何很强
